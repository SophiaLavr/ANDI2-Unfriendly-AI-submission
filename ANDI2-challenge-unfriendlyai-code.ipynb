{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8831861,"sourceType":"datasetVersion","datasetId":5314235}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"public_data_path = '/kaggle/input/anomalous-diffusion-challenge/' # make sure the folder has this name or change it","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.stats import anderson\nfrom scipy.signal import find_peaks, argrelextrema\n\nnp.random.seed(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fill_short_segments_np(labels, minseq=3):\n    \"\"\"\n    Fills short segments in the labels array with neighboring values.\n    \n    Args:\n    labels (numpy.array): Input array of labels\n    minseq (int): Minimum sequence length to keep unchanged (default: 3)\n    \n    Returns:\n    numpy.array: Modified array with short segments filled\n    \"\"\"\n    \n    # Recursive call for larger minseq values\n    if minseq > 2:\n        labels = fill_short_segments_np(labels, minseq=minseq - 1)\n    \n    # Create a copy of the input array to avoid modifying the original\n    labels = labels.copy()\n    n = len(labels)\n    \n    # Find the change points (boundaries between different labels)\n    change_points = np.where(np.diff(labels) != 0)[0] + 1\n    \n    # Split the array into segments based on change points\n    segments = np.split(np.arange(n), change_points)\n    \n    for segment in segments:\n        # If segment length is less than minseq, fill it with neighboring values\n        if len(segment) < minseq:\n            # Determine left and right neighbor indices\n            left_neighbor_idx = segment[0] - 1 if segment[0] > 0 else None\n            right_neighbor_idx = segment[-1] + 1 if segment[-1] < n - 1 else None\n            \n            # Choose fill value based on available neighbors\n            if left_neighbor_idx is not None:\n                fill_value = labels[left_neighbor_idx]\n            elif right_neighbor_idx is not None:\n                fill_value = labels[right_neighbor_idx]\n            else:\n                # If no neighbors are available, leave the segment unchanged\n                continue\n            \n            # Fill the segment with the chosen value\n            labels[segment] = fill_value\n    \n    return labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fill_short_segments(labels, minseq=3, fill_value=None):\n    \"\"\"\n    Fills short segments in the labels array with neighboring values.\n    \n    Args:\n    labels (pd.Series or np.ndarray): Input array of labels.\n    minseq (int): Minimum length of segment to keep unchanged.\n    fill_value (Any, optional): Value to fill. If None, all short segments are filled.\n    \n    Returns:\n    pd.Series or np.ndarray: Array with filled short segments.\n    \"\"\"\n    # Recursive call for larger minseq values\n    if minseq > 2:\n        labels = fill_short_segments(labels, minseq=minseq - 1, fill_value=fill_value)    \n    \n    # Create a copy of the input array to avoid modifying the original\n    labels = labels.copy()\n    n = len(labels)\n    \n    # Determine if the input is a pandas Series\n    is_pandas = isinstance(labels, pd.Series)\n    \n    # Find the change points (boundaries between different labels)\n    change_points = np.where(np.diff(labels) != 0)[0] + 1\n    \n    # Split the array into segments based on change points\n    segments = np.split(np.arange(n), change_points)\n    \n    for segment in segments:\n        # If segment length is less than minseq, check if it needs to be filled\n        if len(segment) < minseq:\n            current_value = labels.iloc[segment[0]] if is_pandas else labels[segment[0]]\n            \n            # If fill_value is not specified or current value equals fill_value\n            if fill_value is None or current_value == fill_value:\n                left_neighbor_idx = segment[0] - 1 if segment[0] > 0 else None\n                right_neighbor_idx = segment[-1] + 1 if segment[-1] < n - 1 else None\n                \n                # Choose fill value based on available neighbors\n                if is_pandas:\n                    if left_neighbor_idx is not None and left_neighbor_idx in labels.index:\n                        new_value = labels.iloc[left_neighbor_idx]\n                    elif right_neighbor_idx is not None and right_neighbor_idx in labels.index:\n                        new_value = labels.iloc[right_neighbor_idx]\n                    else:\n                        continue\n                else:\n                    if left_neighbor_idx is not None:\n                        new_value = labels[left_neighbor_idx]\n                    elif right_neighbor_idx is not None:\n                        new_value = labels[right_neighbor_idx]\n                    else:\n                        continue\n                \n                # Fill the segment\n                if is_pandas:\n                    labels.iloc[segment] = new_value\n                else:\n                    labels[segment] = new_value\n    \n    return labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#################### The KEY FUNCTION of our METHOD ##############################################################\ndef RowHurst(X1, X2, n):\n    \"\"\"\n    Compute the AD exponent for each row of the input matrices.\n\n    Args:\n    X1 (numpy.ndarray): SD for 1 time step \n    X2 (numpy.ndarray): SD for n time steps \n    n (int): number of time steps for X2\n\n    Returns:\n    numpy.ndarray: Array of AD exponents for each row\n    \"\"\"\n    # Computing the AD exponent\n    res = np.log(np.nanmedian(X1, axis=1) / np.nanmedian(X2, axis=1)) / np.log(n)\n    return res\n#################### The end of KEY FUNCTION of our METHOD ##############################################################\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def anderson_statistic(group):\n    \"\"\"\n    Compute the Anderson-Darling test statistic for a group of data.\n\n    Args:\n    group (pandas.Series or numpy.array): Input data group\n\n    Returns:\n    float or np.NaN: Anderson-Darling test statistic or NaN if insufficient data\n    \"\"\"\n    # Check if there are at least 30 non-NaN values in the group\n    if len(group.dropna()) < 30:\n        return np.NaN\n    \n    # Perform Anderson-Darling test for exponential distribution\n    result = anderson(group.dropna(), dist='expon')\n    \n    # Return the test statistic\n    return result.statistic","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_freedom(current_class, alpha):\n    \"\"\"\n    Determine the state of freedom based on the current class, alpha value, and model type prediction.\n\n    Args:\n    current_class (int): The current class of the particle motion \n    alpha (float): The anomalous diffusion exponent\n\n    Returns:\n    int: The state of freedom (0, 1, 2, or 3)\n\n    Note: This function uses a global variable 'model_type_prediction'.\n    \"\"\"\n    # Determine the initial state based on alpha value\n    if alpha > 1.89:\n        st = 3\n    else:\n        st = 2\n    \n    # Special case: Immobile traps\n    if current_class == 0 and model_type_prediction == 'immobile_traps':   \n        return 0\n    # Special case: Confinement \n    elif (alpha < 0.6 or exp==5 and current_class==1) and model_type_prediction == 'confinement':      \n        return 1\n    # Default case: Return the initial state\n    else:\n        return st","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom scipy.signal import find_peaks\nfrom scipy.signal import argrelextrema\n\ndef visualize_rolling_stats(df, big=1, num_bins=30, min_exp=-6, max_exp=5, peak_height=0.05):\n    \"\"\"\n    Visualize rolling statistics data with both normal and density histograms, peaks, and minima for all relevant columns.\n    \n    :param df: DataFrame containing the data\n    :param big: Scaling factor for the data (default: 1)\n    :param num_bins: Number of bins for the histogram (default: 80)\n    :param min_exp: Minimum exponent for log-scale bins (default: -7)\n    :param max_exp: Maximum exponent for log-scale bins (default: 6)\n    :param peak_height: Minimum height for peak detection (default: 0.05)\n    \"\"\"\n    # Filter columns\n    columns = [col for col in df.columns if 'rolling' in col and 'alpha' not in col]\n    \n    # Calculate number of rows needed (1 row per statistic, 2 columns)\n    num_plots = len(columns)\n    num_rows = num_plots\n\n    # Create logarithmically spaced bins\n    bins = np.exp(np.linspace(min_exp, max_exp, num_bins))\n    \n    # Create subplots\n    fig, axs = plt.subplots(num_rows, 2, figsize=(20, 5*num_rows), squeeze=False)\n    fig.suptitle('Rolling Statistics Visualization', fontsize=16)\n\n    for idx, column_name in enumerate(columns):\n        data = big * df[column_name].dropna()\n\n        # Normal histogram (density=False)\n        ax_normal = axs[idx, 0]\n        hist_normal, bin_edges = np.histogram(data, bins=bins, density=False)\n        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n        \n        # Find peaks and minima for normal histogram\n        peaks_normal, _ = find_peaks(hist_normal, height=max(hist_normal) * peak_height)\n        minima_normal = argrelextrema(hist_normal, np.less, order=2)[0]\n        \n        ax_normal.set_xscale('log')\n        ax_normal.stairs(hist_normal, bin_edges, fill=True, alpha=0.7, label='Histogram')\n        ax_normal.plot(bin_centers[peaks_normal], hist_normal[peaks_normal], \"x\", color='red', label='Peaks')\n        ax_normal.plot(bin_centers[minima_normal], hist_normal[minima_normal], \"o\", color='green', label='Minima')\n        ax_normal.set_title(f'{column_name} (Absolute Frequency)  {bin_centers[minima_normal]}', fontsize=10)\n        ax_normal.set_xlabel('Value (log scale)', fontsize=8)\n        ax_normal.set_ylabel('Frequency', fontsize=8)\n        ax_normal.tick_params(axis='both', which='major', labelsize=6)\n        ax_normal.tick_params(axis='both', which='minor', labelsize=4)\n        ax_normal.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n        if idx == 0:\n            ax_normal.legend(fontsize=6)\n\n        # Density histogram (density=True)\n        ax_density = axs[idx, 1]\n        hist_density, _ = np.histogram(data, bins=bins, density=True)\n        \n        # Find peaks and minima for density histogram\n        peaks_density, _ = find_peaks(hist_density, height=peak_height)\n        minima_density = argrelextrema(hist_density, np.less, order=2)[0]\n        \n        ax_density.set_xscale('log')\n        ax_density.stairs(hist_density, bin_edges, fill=True, alpha=0.7, label='Histogram')\n        ax_density.plot(bin_centers[peaks_density], hist_density[peaks_density], \"x\", color='red', label='Peaks')\n        ax_density.plot(bin_centers[minima_density], hist_density[minima_density], \"o\", color='green', label='Minima')\n        \n        ax_density.set_title(f'{column_name} (Density)  {bin_centers[minima_density]}', fontsize=10)\n        ax_density.set_xlabel('Value (log scale)', fontsize=8)\n        ax_density.set_ylabel('Density', fontsize=8)\n        ax_density.tick_params(axis='both', which='major', labelsize=6)\n        ax_density.tick_params(axis='both', which='minor', labelsize=4)\n        ax_density.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n        if idx == 0:\n            ax_density.legend(fontsize=6)\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_traj(big_df1):\n    \"\"\"\n    Visualize trajectories from big_df1 on a rectangular field defined by big_df1['x'] and big_df1['y'].\n    \n    Args:\n    big_df1 (pandas.DataFrame): DataFrame containing 'x', 'y', and 'traj_idx' columns\n    \n    Returns:\n    None (displays the plot)\n    \"\"\"\n    # Create a new figure and axis with specified size\n    fig, ax = plt.subplots(figsize=(12, 8))\n    \n    # Create a scatter plot\n    scatter = ax.scatter(big_df1['x'], big_df1['y'], \n                         c=big_df1['traj_idx'],  # Color points by trajectory index\n                         cmap='viridis',  # Use viridis colormap\n                         s=4,  # Set point size\n                         alpha=0.6)  # Set point transparency\n    \n    # Add a colorbar\n    cbar = plt.colorbar(scatter)\n    cbar.set_label('Trajectory Index', rotation=270, labelpad=15)\n    \n    # Set axis labels and title\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Visualization of Trajectories on X-Y Field')\n    \n    # Add a grid for better orientation\n    ax.grid(True, linestyle='--', alpha=0.7)\n    \n    # Adjust layout and display the plot\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.signal import find_peaks, argrelextrema\n\ndef divide_single_statistic_1only(df, statistic_type='median', statistic_name='9', statistic_density=False, num_bins=30, min_exp=-6, max_exp=4, peak_height=0.05, visualize=False):\n    \"\"\"\n    Visualize a single statistic with both normal and density histograms, peaks, and minima for the entire dataframe.\n    \n    Args:\n    df (pandas.DataFrame): DataFrame containing the data\n    statistic_type (str): Type of statistic (default: 'median')\n    statistic_name (str): Name of the column to visualize (default: '9')\n    statistic_density (bool): Whether to use density for histogram (default: False)\n    num_bins (int): Number of bins for the histogram (default: 30)\n    min_exp (float): Minimum exponent for log-scale bins (default: -6)\n    max_exp (float): Maximum exponent for log-scale bins (default: 4)\n    peak_height (float): Minimum height for peak detection (default: 0.05)\n    visualize (bool): Whether to visualize the results (default: False)\n\n    Returns:\n    numpy.ndarray: Labels array based on the analysis\n    \"\"\"\n    # Create logarithmically spaced bins\n    bins = np.exp(np.linspace(min_exp, max_exp, num_bins))\n    labels = df['rolling_'+statistic_type+'4_'+statistic_name] * 0    \n    \n    # Create subplots if visualization is enabled\n    if visualize:\n        fig, (ax_normal, ax_density) = plt.subplots(1, 2, figsize=(20, 5))\n        fig.suptitle(f'Visualization of {statistic_name}', fontsize=16)\n    \n    # Iterate through different rolling window sizes\n    for k in [1, 3, 4, 2]:\n        data = df['rolling_'+statistic_type+str(k)+'_'+statistic_name]\n    \n        # Compute histogram\n        hist_normal, bin_edges = np.histogram(data, bins=bins, density=statistic_density)\n        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n        \n        # Find peaks in the histogram\n        peaks_normal, _ = find_peaks(hist_normal, height=max(hist_normal) * peak_height)\n        \n        if len(peaks_normal) == 2:\n            # If exactly two peaks are found\n            left_peak, right_peak = peaks_normal\n            # Find the minimum between the two peaks\n            segment = hist_normal[left_peak:right_peak+1]\n            min_index = np.argmin(segment) + left_peak \n            min_value = bin_centers[min_index]\n            \n            print(f\"k={k} minimum={min_value} peaks {bin_centers[peaks_normal]}   {'rolling_'+statistic_type+str(k)+'_'+statistic_name}\")\n            \n            if visualize:\n                # Visualize the histogram, peaks, and minimum\n                ax_normal.set_xscale('log')\n                ax_normal.stairs(hist_normal, bin_edges, fill=True, alpha=0.7, label='Histogram')\n                ax_normal.plot(bin_centers[peaks_normal], hist_normal[peaks_normal], \"x\", color='red', label='Peaks')\n                ax_normal.plot(bin_centers[min_index], hist_normal[min_index], \"o\", color='green', label='Minima')\n                ax_normal.set_title(f'{statistic_name} min= {min_value}', fontsize=12)\n                ax_normal.set_xlabel('Value (log scale)', fontsize=10)\n                ax_normal.set_ylabel('Frequency', fontsize=10)\n                ax_normal.tick_params(axis='both', which='major', labelsize=8)\n                ax_normal.tick_params(axis='both', which='minor', labelsize=6)\n                ax_normal.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n                ax_normal.legend(fontsize=8)\n                plt.show()\n            \n            # Create labels based on the minimum value\n            labels = fill_short_segments((data > min_value).astype(int), minseq=3, fill_value=0)\n            labels = fill_short_segments(labels, minseq=3, fill_value=1)\n            \n            break\n    \n    return labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_single_statistic(df, statistic_name, big=1, num_bins=30, min_exp=-6, max_exp=4, peak_height=0.05):\n    \"\"\"\n    Visualize a single statistic with both normal and density histograms, peaks, and minima for the entire dataframe.\n    \n    :param df: DataFrame containing the data\n    :param statistic_name: Name of the column to visualize\n    :param big: Scaling factor for the data (default: 1)\n    :param num_bins: Number of bins for the histogram (default: 30)\n    :param min_exp: Minimum exponent for log-scale bins (default: -6)\n    :param max_exp: Maximum exponent for log-scale bins (default: 4)\n    :param peak_height: Minimum height for peak detection (default: 0.05)\n    \"\"\"\n    # Create logarithmically spaced bins\n    bins = np.exp(np.linspace(min_exp, max_exp, num_bins))\n    \n    # Create subplots\n    fig, (ax_normal, ax_density) = plt.subplots(1, 2, figsize=(20, 5))\n    fig.suptitle(f'Visualization of {statistic_name}', fontsize=16)\n    \n    # Prepare data\n    data = big * df[statistic_name].dropna()\n    \n    # Normal histogram (density=False)\n    hist_normal, bin_edges = np.histogram(data, bins=bins, density=False)\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n    \n    # Find peaks and minima for normal histogram\n    peaks_normal, _ = find_peaks(hist_normal, height=max(hist_normal) * peak_height)\n    minima_normal = argrelextrema(hist_normal, np.less, order=2)[0]\n    \n    ax_normal.set_xscale('log')\n    ax_normal.stairs(hist_normal, bin_edges, fill=True, alpha=0.7, label='Histogram')\n    ax_normal.plot(bin_centers[peaks_normal], hist_normal[peaks_normal], \"x\", color='red', label='Peaks')\n    ax_normal.plot(bin_centers[minima_normal], hist_normal[minima_normal], \"o\", color='green', label='Minima')\n    ax_normal.set_title(f'{statistic_name} (Absolute Frequency)', fontsize=12)\n    ax_normal.set_xlabel('Value (log scale)', fontsize=10)\n    ax_normal.set_ylabel('Frequency', fontsize=10)\n    ax_normal.tick_params(axis='both', which='major', labelsize=8)\n    ax_normal.tick_params(axis='both', which='minor', labelsize=6)\n    ax_normal.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n    ax_normal.legend(fontsize=8)\n    \n    # Density histogram (density=True)\n    hist_density, _ = np.histogram(data, bins=bins, density=True)\n    \n    # Find peaks and minima for density histogram\n    peaks_density, _ = find_peaks(hist_density, height=peak_height)\n    minima_density = argrelextrema(hist_density, np.less, order=2)[0]\n    \n    ax_density.set_xscale('log')\n    ax_density.stairs(hist_density, bin_edges, fill=True, alpha=0.7, label='Histogram')\n    ax_density.plot(bin_centers[peaks_density], hist_density[peaks_density], \"x\", color='red', label='Peaks')\n    ax_density.plot(bin_centers[minima_density], hist_density[minima_density], \"o\", color='green', label='Minima')\n    \n    ax_density.set_title(f'{statistic_name} (Density)', fontsize=12)\n    ax_density.set_xlabel('Value (log scale)', fontsize=10)\n    ax_density.set_ylabel('Density', fontsize=10)\n    ax_density.tick_params(axis='both', which='major', labelsize=8)\n    ax_density.tick_params(axis='both', which='minor', labelsize=6)\n    ax_density.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n    ax_density.legend(fontsize=8)\n    \n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def divide_single_statistic(df, statistic_type='mean', statistic_name='15', statistic_density='True', num_bins=30, min_exp=-6, max_exp=4, peak_height=0.05):\n    \"\"\"\n    Analyze a single statistic from the DataFrame, find local minima in its distribution,\n    and create labels based on the rightmost minimum.\n\n    Args:\n    df (pandas.DataFrame): DataFrame containing the data\n    statistic_type (str): Type of statistic (default: 'mean')\n    statistic_name (str): Name of the column to analyze (default: '15')\n    statistic_density (str): Whether to use density for histogram ('True' or 'False')\n    num_bins (int): Number of bins for the histogram (default: 30)\n    min_exp (float): Minimum exponent for log-scale bins (default: -6)\n    max_exp (float): Maximum exponent for log-scale bins (default: 4)\n    peak_height (float): Minimum height for peak detection (unused in this function)\n\n    Returns:\n    numpy.ndarray: Labels array based on the analysis\n    \"\"\"\n    # Create logarithmically spaced bins\n    bins = np.exp(np.linspace(min_exp, max_exp, num_bins))\n    \n    # Initialize labels array\n    labels = df['rolling_'+statistic_type+'4_'+statistic_name] * 0\n    \n    # Iterate through different rolling window sizes in reverse order\n    for k in [4, 3, 2, 1]:\n        # Extract data for the current window size\n        data = df['rolling_'+statistic_type+str(k)+'_'+statistic_name]\n        \n        # Compute histogram\n        hist, bin_edges = np.histogram(data, bins=bins, density=statistic_density)\n        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n        \n        # Find local minima in the histogram\n        minima = argrelextrema(hist, np.less, order=2)[0]    \n        extrems = bin_centers[minima]\n        \n        if len(extrems):\n            # If local minima are found, use the rightmost one as a threshold\n            threshold = extrems[-1]\n            \n            # Create labels based on whether the data is above the threshold\n            labels = fill_short_segments((df['rolling_'+statistic_type+str(k)+'_'+statistic_name] > threshold).astype(int), 3)\n            \n            print(f'k={k} extr={threshold}')\n            break\n    \n    return labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_rolling_median(big_df1):\n    \"\"\"\n    Visualize the 'class' values from big_df1 on a rectangular field defined by big_df1['x'] and big_df1['y'].\n    \n    Args:\n    big_df1 (pandas.DataFrame): DataFrame containing 'x', 'y', and 'class' columns\n    \n    Returns:\n    None (displays the plot)\n    \"\"\"\n    # Create a new figure and axis with specified size\n    fig, ax = plt.subplots(figsize=(12, 8))\n    \n    # Create a scatter plot\n    scatter = ax.scatter(big_df1['x'], big_df1['y'], \n                         c=big_df1['class'],  # Color points by class\n                         cmap='plasma',  # Use plasma colormap\n                         s=5,  # Set point size\n                         alpha=0.6)  # Set point transparency\n    \n    # Add a colorbar\n    cbar = plt.colorbar(scatter)\n    cbar.set_label('Class', rotation=270, labelpad=15)\n    \n    # Set axis labels and title\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Visualization of Classes on X-Y Field')\n    \n    # Add a grid for better orientation\n    ax.grid(True, linestyle='--', alpha=0.7)\n    \n    # Adjust layout and display the plot\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\ndef get_dominant_gaussian_params(data_series, n_components=2, class_name='K', visualize=True):\n    \"\"\"\n    Function to determine parameters of the dominant Gaussian from a mixture of Gaussians\n    and visualize the data distribution.\n    \n    Parameters:\n    data_series (pandas.Series): Series of 'alpha' data for a specific class.\n    n_components (int): Number of components in the Gaussian mixture.\n    class_name (str, optional): Class name for the plot title.\n    visualize (bool): Flag to enable/disable visualization.\n    \n    Returns:\n    tuple: (mu, sigma) of the dominant Gaussian. Returns (None, None) if an error occurs.\n    \"\"\"\n    try:\n        # Remove NaN values and convert to numpy array\n        data = data_series.dropna().values.reshape(-1, 1)\n        \n        # Check if there's enough data for GMM\n        if len(data) < 2:\n            print(\"Not enough data for GMM.\")\n            return None, None\n        \n        # Fit GMM with the specified number of components\n        gmm = GaussianMixture(n_components=n_components, random_state=42)\n        gmm.fit(data)\n        \n        # Get parameters of all Gaussians\n        means = gmm.means_.flatten()\n        sigmas = np.sqrt(gmm.covariances_.flatten())\n        weights = gmm.weights_\n        \n        # Determine which Gaussian has the highest weight\n        dominant_index = np.argmax(weights)\n        mu, sigma = means[dominant_index], sigmas[dominant_index]\n        \n        if visualize:\n            plt.figure(figsize=(10, 6))\n            \n            # Data histogram\n            plt.hist(data, bins=30, density=True, alpha=0.7, color='skyblue')\n            \n            # Plot all Gaussians\n            x = np.linspace(data.min(), data.max(), 100)\n            for i in range(n_components):\n                plt.plot(x, weights[i]*norm.pdf(x, means[i], sigmas[i]), \n                         f'C{i}-', lw=2, label=f'Gaussian {i+1}')\n            \n            # Highlight the dominant Gaussian and median\n            plt.axvline(mu, color='k', linestyle='--', lw=2, label='μ of dominant Gaussian')\n            plt.axvline(np.median(data), color='g', linestyle='--', lw=2, label='median')\n            \n            # Configure the plot\n            plt.title(f'μ = {mu:.5f}, median = {np.median(data):.5f}, σ = {sigma:.5f}')\n            plt.xlabel(class_name)\n            plt.ylabel('Density')\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            plt.show()\n        \n        return mu, sigma\n    \n    except Exception as e:\n        print(f\"An error occurred while processing the data: {e}\")\n        return None, None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom scipy.stats import truncnorm\nfrom scipy.optimize import minimize\n\ndef truncated_normal_pdf(x, mu, sigma, a=0, b=2):\n    \"\"\"Calculate the probability density function of a truncated normal distribution.\"\"\"\n    return truncnorm.pdf(x, (a - mu) / sigma, (b - mu) / sigma, loc=mu, scale=sigma)\n\ndef neg_log_likelihood(params, data, a=0, b=2):\n    \"\"\"Calculate the negative log-likelihood for the truncated normal distribution.\"\"\"\n    mu, sigma = params\n    return -np.sum(np.log(truncated_normal_pdf(data, mu, sigma, a, b) + 1e-10))\n\ndef get_bounded_gaussian_params(data_series, class_name='Alpha', visualize=True, a=0, b=2):\n    \"\"\"\n    Function to determine parameters of a bounded normal distribution\n    and visualize the data distribution.\n    \n    Parameters:\n    data_series (pandas.Series): Series of 'alpha' data for a specific class.\n    class_name (str, optional): Class name for the plot title.\n    visualize (bool): Flag to enable/disable visualization.\n    a (float): Lower bound of the distribution.\n    b (float): Upper bound of the distribution.\n    \n    Returns:\n    tuple: (mu, sigma) of the bounded normal distribution.\n    \"\"\"\n    try:\n        # Remove NaN values\n        data = data_series.dropna().values\n        \n        # Check if there's enough data\n        if len(data) < 2:\n            print(\"Not enough data to estimate parameters.\")\n            return None, None\n        \n        # Initial guess for parameters\n        initial_mu = np.mean(data)\n        initial_sigma = np.std(data)\n        \n        # Optimize to find parameters\n        result = minimize(neg_log_likelihood, [initial_mu, initial_sigma], \n                          args=(data, a, b), \n                          bounds=[(a, b), (1e-5, None)])\n        \n        mu, sigma = result.x\n        \n        if visualize:\n            plt.figure(figsize=(10, 6))\n            \n            # Data histogram\n            plt.hist(data, bins=30, density=True, alpha=0.7, color='skyblue')\n            \n            # Plot fitted distribution\n            x = np.linspace(data.min(), data.max(), 100)\n            y = truncated_normal_pdf(x, mu, sigma, a, b)\n            plt.plot(x, y, 'r-', lw=2, label='Fitted distribution')\n            plt.axvline(np.median(data), color='g', linestyle='--', lw=2, label='median')\n            plt.axvline(mu, color='k', linestyle='--', lw=2, label='μ')\n            \n            plt.title(f'μ = {mu:.5f}, median = {np.median(data):.5f}, σ = {sigma:.5f}')\n            plt.xlabel(class_name)\n            plt.ylabel('Density')\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            plt.show()\n        \n        return mu, sigma\n    \n    except Exception as e:\n        print(f\"An error occurred while processing the data: {e}\")\n        return None, None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import cKDTree\n\ndef confined_circles5(big_df1, rol_stat='rolling_median3_7', radius=5, step=0.25, visualization=False, min_new_points=5):\n    # Classify points for confined exp 5 (Fast)\n    big_df1.loc[:, 'class'] = 0\n    threshold_low = big_df1[rol_stat].quantile(0.07)\n    threshold_high = big_df1[rol_stat].quantile(0.80)\n    big_df1.loc[big_df1[rol_stat] <= threshold_low, 'class'] = 1\n    big_df1.loc[big_df1[rol_stat] >= threshold_high, 'class'] = 2\n\n    # Filter points of class 1 and class 2\n    class_1_points = big_df1[big_df1['class'] == 1]\n    class_2_points = big_df1[big_df1['class'] == 2]\n\n    # Create KD-trees for fast neighbor search\n    tree_class_1 = cKDTree(class_1_points[['x', 'y']])\n    tree_class_2 = cKDTree(class_2_points[['x', 'y']])\n\n    # Create grid points for iteration\n    x_min, x_max = big_df1['x'].min(), big_df1['x'].max()\n    y_min, y_max = big_df1['y'].min(), big_df1['y'].max()\n    x_range = np.arange(x_min, x_max + step, step)\n    y_range = np.arange(y_min, y_max + step, step)\n    grid_points = np.array(np.meshgrid(x_range, y_range)).T.reshape(-1, 2)\n\n    # Calculate number of points for each potential circle\n    circle_data = []\n    for cx, cy in grid_points:\n        covered_class_1_indices = tree_class_1.query_ball_point([cx, cy], radius)\n        covered_class_2_indices = tree_class_2.query_ball_point([cx, cy], radius)\n        covered_class_1_points = len(covered_class_1_indices)\n        covered_class_2_points = len(covered_class_2_indices)\n        score = covered_class_1_points - covered_class_2_points ** 2\n        if score >= min_new_points:\n            circle_data.append((cx, cy, score, set(covered_class_1_indices)))\n\n    # Sort circles by number of covered points\n    circle_data.sort(key=lambda x: x[2], reverse=True)\n\n    # Select circles\n    selected_centers = []\n    covered_points = set()\n\n    def circles_overlap(c1, c2, threshold=1.5):\n        return np.sqrt((c1[0] - c2[0])**2 + (c1[1] - c2[1])**2) < threshold * radius\n\n    for cx, cy, _, indices in circle_data:\n        # Check for overlap with existing circles\n        if any(circles_overlap((cx, cy), center) for center in selected_centers):\n            continue\n        \n        new_points = indices - covered_points\n        if len(new_points) >= min_new_points:\n            selected_centers.append((cx, cy))\n            covered_points.update(new_points)\n\n    # Visualization of circles (if enabled)\n    if visualization:\n        plt.figure(figsize=(12, 8))\n        scatter = plt.scatter(big_df1['x'], big_df1['y'], c=big_df1['class'], cmap='viridis', alpha=0.6)\n        plt.colorbar(scatter, label='Class')\n        plt.title('Circles Covering Maximum Class 1 Points and Excluding Class 2 Points')\n        plt.xlabel('x')\n        plt.ylabel('y')\n        for center in selected_centers:\n            circle = plt.Circle((center[0], center[1]), radius, fill=False, color='red')\n            plt.gca().add_artist(circle)\n        plt.show()\n\n    # Assign class 1 to points falling within the circles\n    big_df1.loc[:, 'class'] = 0\n    tree_all_points = cKDTree(big_df1[['x', 'y']])\n    for center in selected_centers:\n        indices = tree_all_points.query_ball_point(center, radius)\n        big_df1.loc[big_df1.index[indices], 'class'] = 1\n\n    print(f\"Found {len(selected_centers)} circle centers satisfying conditions.\")\n\n    # Apply fill_short_segments to smooth out classification\n    labels = fill_short_segments(big_df1['class'], minseq=7, fill_value=1)\n    labels = fill_short_segments(labels, minseq=7, fill_value=0)\n\n    return labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def confined_circles(big_df1, rol_stat='rolling_median3_7', radius=5, visualize=False):\n    \n    # Classify points for confined exp 6 (very slow. TODO use algorithm confined_circles5)\n    big_df1.loc[:,'class'] = 0\n    threshold_low = big_df1[rol_stat].quantile(0.07)\n    threshold_high = big_df1[rol_stat].quantile(0.93)\n    big_df1.loc[big_df1[rol_stat] <= threshold_low, 'class'] = 1\n    big_df1.loc[big_df1[rol_stat] >= threshold_high, 'class'] = 2\n\n\n    # Filter points of class 1 and class 2\n    class_1_points = big_df1[big_df1['class'] == 1]\n    class_2_points = big_df1[big_df1['class'] == 2]\n    class_01_points = big_df1[big_df1['class'] < 2]\n\n    # Iteratively add circles\n    selected_centers = []\n    selected_radii = []\n    while len(selected_centers) < 20 and not class_1_points.empty:\n        max_covered_points = -1000\n        best_center = None\n        print(f'centers {len(selected_centers)}')\n\n        # Iterate through all points of class 0 and 1 to choose the optimal circle center\n        for _, point in class_01_points.iterrows():\n            cx, cy = point['x'], point['y']\n            # Calculate distances to all points of class 1 and class 2\n            distances_to_class_1 = np.sqrt((class_1_points['x'] - cx)**2 + (class_1_points['y'] - cy)**2)\n            distances_to_class_2 = np.sqrt((class_2_points['x'] - cx)**2 + (class_2_points['y'] - cy)**2)\n            # Count class 1 points within the circle\n            covered_class_1_points = np.sum(distances_to_class_1 <= radius)\n            # Count class 2 points within the circle\n            covered_class_2_points = np.sum(distances_to_class_2 <= radius)\n            # Greedy algorithm: choose the circle that covers the maximum number of class 1 points and minimum number of class 2 points\n            if covered_class_1_points - covered_class_2_points * 5 > max_covered_points:\n                max_covered_points = covered_class_1_points - covered_class_2_points * 5\n                best_center = (cx, cy)\n\n        if best_center:\n            selected_centers.append(best_center)\n            selected_radii.append(radius)\n            # Remove covered class 1 points\n            distances = np.sqrt((class_1_points['x'] - best_center[0])**2 + (class_1_points['y'] - best_center[1])**2)\n            class_1_points = class_1_points[distances >= radius]\n            if max_covered_points < 3:\n                break\n\n    if len(selected_centers) < 20:\n        print(f\"Error: Could not find 20 circle centers satisfying conditions. Found {len(selected_centers)} centers.\")\n    else:\n        print(\"Found 20 circle centers satisfying conditions.\")\n\n\n    # Assign class 1 to points falling within the circles\n    big_df1.loc[:,'class'] = 0\n    for center in selected_centers:\n        distances_to_center = np.sqrt((big_df1['x'] - center[0])**2 + (big_df1['y'] - center[1])**2)\n        big_df1.loc[distances_to_center <= radius, 'class'] = 1\n    \n    return fill_short_segments(big_df1['class'], 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def div_by_rollstat(big_df1, rol_stat='rolling_mean1_11'):\n    \"\"\"\n    Classify points of multy_state model based on rolling statistics and rolling_alpha_15.\n\n    Parameters:\n    big_df1 (DataFrame): Input dataframe containing trajectory data.\n    rol_stat (str): Column name for the rolling statistic to use. Default is 'rolling_mean1_11'.\n\n\n    Returns:\n    array: Classified and filled array of labels.\n    \"\"\"\n    # Define threshold for classification\n    extrems = np.array([1.2])\n\n    # Classify points based on rolling statistic and additional condition\n    fov_class = ((big_df1[rol_stat] < extrems[0]).astype(int) + \n                 ((big_df1[rol_stat] < extrems[0]) & (big_df1['rolling_alpha_15'] > 1.2)).astype(int))\n\n    # Fill short segments to smooth classification\n    return fill_short_segments(fov_class, 6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def div_by_rollstat9(big_df1):\n    \"\"\"\n    Classify points of multy_state model based on rolling statistics and rolling_alpha_15.\n\n    Parameters:\n    big_df1 (DataFrame): Input dataframe containing trajectory data.\n\n\n    Returns:\n    array: Classified and filled array of labels.\n    \"\"\"\n    # Get initial classification based on a single statistic\n    fov_class0 = divide_single_statistic(big_df1)\n    \n    # Classify points based on rolling alpha value\n    fov_class = (big_df1['rolling_alpha_15'] > 1.35).astype(int) + 1 \n    \n    # Combine classifications if fov_class0 has non-zero values\n    if fov_class0.max() > 0:\n        fov_class = fov_class * fov_class0\n    \n    # Fill short segments to smooth classification\n    return fill_short_segments(fov_class, 6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_conf(big_df1, rol_stat='rolling_median3_9', radius=9, step=5.0, visualization=False):\n    \"\"\"\n    Check for confined motion in trajectory data.\n\n    Parameters:\n    big_df1 (DataFrame): Input dataframe containing trajectory data.\n    rol_stat (str): Column name for the rolling statistic. Default is 'rolling_median3_15'.\n    radius (float): Radius for circular neighborhood search. Default is 9.\n    step (float): Step size for grid creation. Default is 5.0.\n    visualization (bool): Flag for visualization (unused in this function). Default is False.\n\n    Returns:\n    bool: True if confined motion is detected, False otherwise.\n    \"\"\"\n    big_df1 = big_df1.copy()\n    big_df1.loc[:, 'class'] = 0\n    print(\"Points in fov = \", big_df1['class'].count())\n\n    # Classify points based on rolling statistic\n    threshold_low = big_df1[rol_stat].nsmallest(1000).iloc[-1]\n    threshold_high = big_df1[rol_stat].nlargest(1000).iloc[-1]\n    big_df1.loc[big_df1[rol_stat] <= threshold_low, 'class'] = 1\n    big_df1.loc[big_df1[rol_stat] >= threshold_high, 'class'] = 2\n\n    # Filter points of class 1 and class 2\n    class_1_points = big_df1[big_df1['class'] == 1]\n    class_2_points = big_df1[big_df1['class'] == 2]\n\n    # Create KD-trees for fast neighbor search\n    tree_class_1 = cKDTree(class_1_points[['x', 'y']])\n    tree_class_2 = cKDTree(class_2_points[['x', 'y']])\n\n    # Create grid points for iteration\n    x_min, x_max = big_df1['x'].min() + radius, big_df1['x'].max() - radius\n    y_min, y_max = big_df1['y'].min() + radius, big_df1['y'].max() - radius\n    x_range = np.arange(x_min, x_max + step, step)\n    y_range = np.arange(y_min, y_max + step, step)\n    grid_points = np.array(np.meshgrid(x_range, y_range)).T.reshape(-1, 2)\n\n    # Calculate number of points for each potential circle\n    circle1, circle2 = [], []\n    for cx, cy in grid_points:\n        covered_class_1_indices = tree_class_1.query_ball_point([cx, cy], radius)\n        covered_class_2_indices = tree_class_2.query_ball_point([cx, cy], radius)\n        circle1.append(len(covered_class_1_indices))\n        circle2.append(len(covered_class_2_indices))\n\n    # Sort circles by number of covered points\n    circle1.sort(reverse=False)\n    circle2.sort(reverse=False)\n\n    # Find first non-zero count for each class\n    first_non_zero1 = next(i for i, v in enumerate(circle1) if v > 0)\n    first_non_zero2 = next(i for i, v in enumerate(circle2) if v > 0)\n    print(first_non_zero1,first_non_zero2)\n    # Check if the minimum of first non-zero counts is greater than 130 threshold\n    if min(first_non_zero1, first_non_zero2) > 90:\n        return True\n    return False\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_dim(big_df1):\n    \"\"\"\n    Check dimerization.\n\n    :param big_df1: DataFrame containing required columns ('dist', 'rolling_mean1_9', 'rolling_median1_9')\n    :return: True if dimerization conditions are met, False otherwise\n    \"\"\"\n    # Extract 'dist' and 'rolling_mean1_9' columns from the dataframe\n    x = big_df1.dist.values\n    y = big_df1.rolling_mean1_9.values\n    \n    bin_width = 0.5\n    bins = np.arange(start=x.min(), stop=x.max() + bin_width, step=bin_width)\n    indmax = len(bins) - 2\n\n    # Divide x values into bins and calculate mean y values for each bin\n    indices = np.digitize(x, bins)  # Find bin indices for each x value\n    y_0 = np.nanmean(y[indices == 1])  # Mean y value for the first bin\n    y_m = np.nanmean(y[indices == indmax])  # Mean y value for the last bin\n    y_tr = (y_0 + y_m) * 0.5  # Threshold y value\n\n    # Calculate mean y values for all bins\n    y_means = [np.nanmean(y[indices == i]) for i in range(1, len(bins))]\n    y_means = np.array(y_means)\n    y_means = np.nan_to_num(y_means, nan=0.0)  # Replace NaN with 0.0\n\n    if y_0 < y_m:\n        # Find r_min where y_means exceeds y_tr\n        r_min = bins[np.argmax(y_means[1:] > y_tr)+1] if np.any(np.array(y_means) > y_tr) else None\n        \n        # Calculate percentage of points with dist < r_min\n        dist_part = fill_short_segments((big_df1['dist'] < r_min) + 0, 3).sum() / big_df1['dist'].count()\n        print(f\"dist% {dist_part}\")\n        \n        # Calculate percentage of points with rolling_median1_9 < y_tr\n        y_part = fill_short_segments((big_df1['rolling_median1_9'] < y_tr) + 0, 3).sum() / big_df1['dist'].count()\n        print(f\"div% {y_part}\")\n        \n        # Calculate percentage of points satisfying both conditions\n        dist_and_y_part = fill_short_segments((((big_df1['dist'] < r_min) + 0 + (big_df1['rolling_median1_9'] < y_tr) + 0) > 1) + 0, 3).sum() / big_df1['dist'].count()\n        print(f\"d+r% {dist_and_y_part}\")\n        \n    else:\n        # Similar calculations as above, but with reversed inequality signs\n        r_min = bins[np.argmax(y_means[1:] < y_tr)+1] if np.any(np.array(y_means) < y_tr) else None\n        dist_part = fill_short_segments((big_df1['dist'] < r_min) + 0, 3).sum() / big_df1['dist'].count()\n        print(f\"dist% {dist_part}\")\n        y_part = fill_short_segments((big_df1['rolling_median1_9'] > y_tr) + 0, 3).sum() / big_df1['dist'].count()\n        print(f\"div% {y_part}\")\n        dist_and_y_part = fill_short_segments((((big_df1['dist'] < r_min) + 0 + (big_df1['rolling_median1_9'] > y_tr) + 0) > 1) + 0, 3).sum() / big_df1['dist'].count()\n        print(f\"d+r% {dist_and_y_part}\")\n\n    # Return True if conditions are met, otherwise False\n    if dist_and_y_part > 0.03 and dist_and_y_part > y_part * 0.33:\n        return True\n    return False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_trap(df, statistic_type='median', statistic_name='15', statistic_density=False, num_bins=30, min_exp=-6, max_exp=4, peak_height=0.05, visualize=False):\n    \"\"\"\n    Check if trap conditions are met based on histogram peaks.\n\n    :param df: DataFrame containing the data\n    :param statistic_type: Type of statistic to consider ('median', 'mean', etc.)\n    :param statistic_name: Name of the column in the DataFrame to analyze\n    :param statistic_density: Whether to use histogram density (default: False)\n    :param num_bins: Number of bins for the histogram (default: 30)\n    :param min_exp: Minimum exponent for log-scale bins (default: -6)\n    :param max_exp: Maximum exponent for log-scale bins (default: 4)\n    :param peak_height: Minimum height for peak detection (default: 0.05)\n    :param visualize: Whether to visualize histograms (default: False)\n    :return: True if trap conditions are met, False otherwise\n    \"\"\"\n    # Create logarithmically spaced bins\n    bins = np.exp(np.linspace(min_exp, max_exp, num_bins))\n    \n    # Iterate over specific keys\n    for k in [1, 3, 4, 2]:\n        # Construct column name dynamically\n        column_name = 'rolling_' + statistic_type + str(k) + '_' + statistic_name\n        data = df[column_name]\n        \n        # Compute histogram with specified bins and density option\n        hist_normal, bin_edges = np.histogram(data, bins=bins, density=statistic_density)\n        \n        # Find peaks in the histogram\n        peaks_normal, _ = find_peaks(hist_normal, height=max(hist_normal) * peak_height)\n\n        # Check conditions based on detected peaks\n        if len(peaks_normal) != 2:\n            return False\n        \n        if bins[peaks_normal[0]] > 0.05:\n            return False\n\n    return True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_min_distance_column(df):\n    # Create a copy of the DataFrame for updating\n    updated_df = df.copy()\n    \n    # Add a 'dist' column with default values of np.nan\n    updated_df['dist'] = np.nan\n\n    # Iterate through each frame\n    for frame in df['frame'].unique():\n        frame_df = df[df['frame'] == frame]  # Get all rows for the current frame\n        \n        # Iterate through each point in the frame\n        for index, row in frame_df.iterrows():\n            # Calculate distances to all points in the frame\n            distances = np.sqrt((frame_df['x'] - row['x'])**2 + (frame_df['y'] - row['y'])**2)\n            \n            # Set the distance to itself as 8 to exclude it from consideration\n            distances[index] = 8\n            \n            # Find the minimum distance to another point\n            min_distance = distances.min()\n            \n            # Update the 'dist' column with the minimum distance\n            updated_df.at[index, 'dist'] = min_distance\n    \n    return updated_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the number of experiments, number of FOVs (Field of Views), and number of frames\nN_EXP = 12 \nN_FOVS = 30\nN_FRAMES = 200\n\n# We only track 2 in this example\ntrack = 2\n\n# Predefined model type predictions for each experiment\nmodel_type_predictions = []\n\nimport os\n\n# Set up the results directory\npath_results = '/kaggle/working/res/'\nif not os.path.exists(path_results):\n    os.makedirs(path_results)\n\n# Create the folder for the specific track if it doesn't exist\npath_track = path_results + f'track_{track}/'\nif not os.path.exists(path_track):\n    os.makedirs(path_track)\n\n\n# Loop through each experiment\nfor exp in range(N_EXP):\n    # Create a directory for each experiment\n    path_exp = path_track + f'exp_{exp}/'\n    if not os.path.exists(path_exp):\n        os.makedirs(path_exp)\n    \n    # Initialize an empty DataFrame to store all FOV data for this experiment\n    big_df1 = pd.DataFrame()\n\n    # Loop through each Field of View (FOV)\n    for fov in range(N_FOVS):\n        # Read the corresponding csv file from the public data\n        df = pd.read_csv(public_data_path+f'track_2/exp_{exp}/trajs_fov_{fov}.csv')\n        df.sort_values(by=['traj_idx', 'frame'], inplace=True)\n        df['fov'] = fov\n\n        # Add minimum distance column\n        df = add_min_distance_column(df)\n\n        # Calculate differences in x and y coordinates for various time lags\n        df['x_diff'] = df.groupby('traj_idx')['x'].diff(-1).ffill().bfill()\n        df['y_diff'] = df.groupby('traj_idx')['y'].diff(-1).ffill().bfill()\n\n        df['x_diff2'] = df.groupby('traj_idx')['x'].diff(-2).shift(-1).ffill().bfill()\n        df['y_diff2'] = df.groupby('traj_idx')['y'].diff(-2).shift(-1).ffill().bfill()\n\n        df['x_diff3'] = df.groupby('traj_idx')['x'].diff(-3).shift(-1).ffill().bfill()\n        df['y_diff3'] = df.groupby('traj_idx')['y'].diff(-3).shift(-1).ffill().bfill()\n\n        df['x_diff4'] = df.groupby('traj_idx')['x'].diff(-4).shift(-2).ffill().bfill()\n        df['y_diff4'] = df.groupby('traj_idx')['y'].diff(-4).shift(-2).ffill().bfill()\n        \n        # Calculate Mean Squared Displacements (MSD) for different time lags\n        df['msd1'] = (df['x_diff'] ** 2 + df['y_diff'] ** 2) / 4\n        df['msd2'] = (df['x_diff2'] ** 2 + df['y_diff2'] ** 2) / 4\n        df['msd3'] = (df['x_diff3'] ** 2 + df['y_diff3'] ** 2) / 4\n        df['msd4'] = (df['x_diff4'] ** 2 + df['y_diff4'] ** 2) / 4\n        \n        # Calculate rolling medians and means for different window sizes\n        df['rolling_median1_7'] = df.groupby('traj_idx')['msd1'].rolling(window=7, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n        df['rolling_median1_9'] = df.groupby('traj_idx')['msd1'].rolling(window=9, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n        df['rolling_mean1_9'] = df.groupby('traj_idx')['msd1'].rolling(window=9, min_periods=1, center=True).mean().reset_index(level=0, drop=True)\n        df['rolling_median1_11'] = df.groupby('traj_idx')['msd1'].rolling(window=11, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n        df['rolling_mean1_11'] = df.groupby('traj_idx')['msd1'].rolling(window=11, min_periods=1, center=True).mean().reset_index(level=0, drop=True)\n        df['rolling_median1_15'] = df.groupby('traj_idx')['msd1'].rolling(window=15, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n        df['rolling_mean1_15'] = df.groupby('traj_idx')['msd1'].rolling(window=15, min_periods=1, center=True).mean().reset_index(level=0, drop=True)\n        df['rolling_mean1'] = df.groupby('traj_idx')['msd1'].rolling(window=5, min_periods=1, center=True).mean().reset_index(level=0, drop=True)\n        df['rolling_median1'] = df.groupby('traj_idx')['msd1'].rolling(window=5, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n\n        # Repeat rolling calculations for msd2\n        df['rolling_median2_7'] = df.groupby('traj_idx')['msd2'].rolling(window=7, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n        df['rolling_median2_9'] = df.groupby('traj_idx')['msd2'].rolling(window=9, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n        df['rolling_mean2_9'] = df.groupby('traj_idx')['msd2'].rolling(window=9, min_periods=1, center=True).mean().reset_index(level=0, drop=True)\n        df['rolling_median2_11'] = df.groupby('traj_idx')['msd2'].rolling(window=11, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n        df['rolling_mean2_11'] = df.groupby('traj_idx')['msd2'].rolling(window=11, min_periods=1, center=True).mean().reset_index(level=0, drop=True)\n        df['rolling_median2_15'] = df.groupby('traj_idx')['msd2'].rolling(window=15, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n        df['rolling_mean2_15'] = df.groupby('traj_idx')['msd2'].rolling(window=15, min_periods=1, center=True).mean().reset_index(level=0, drop=True)\n        df['rolling_mean2'] = df.groupby('traj_idx')['msd2'].rolling(window=5, min_periods=1, center=True).mean().reset_index(level=0, drop=True)\n        df['rolling_median2'] = df.groupby('traj_idx')['msd2'].rolling(window=5, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n\n        # Repeat rolling calculations for msd3\n        df['rolling_median3_7'] = df.groupby('traj_idx')['msd3'].rolling(window=7, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n        df['rolling_median3_9'] = df.groupby('traj_idx')['msd3'].rolling(window=9, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n        df['rolling_mean3_9'] = df.groupby('traj_idx')['msd3'].rolling(window=9, min_periods=1, center=True).mean().reset_index(level=0, drop=True)\n        df['rolling_median3_11'] = df.groupby('traj_idx')['msd3'].rolling(window=11, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n        df['rolling_mean3_11'] = df.groupby('traj_idx')['msd3'].rolling(window=11, min_periods=1, center=True).mean().reset_index(level=0, drop=True)\n        df['rolling_median3_15'] = df.groupby('traj_idx')['msd3'].rolling(window=15, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n        df['rolling_mean3_15'] = df.groupby('traj_idx')['msd3'].rolling(window=15, min_periods=1, center=True).mean().reset_index(level=0, drop=True)\n        df['rolling_mean3'] = df.groupby('traj_idx')['msd3'].rolling(window=5, min_periods=1, center=True).mean().reset_index(level=0, drop=True)\n        df['rolling_median3'] = df.groupby('traj_idx')['msd3'].rolling(window=5, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n\n        # Repeat rolling calculations for msd4\n        df['rolling_median4_7'] = df.groupby('traj_idx')['msd4'].rolling(window=7, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n        df['rolling_median4_9'] = df.groupby('traj_idx')['msd4'].rolling(window=9, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n        df['rolling_mean4_9'] = df.groupby('traj_idx')['msd4'].rolling(window=9, min_periods=1, center=True).mean().reset_index(level=0, drop=True)\n        df['rolling_median4_11'] = df.groupby('traj_idx')['msd4'].rolling(window=11, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n        df['rolling_mean4_11'] = df.groupby('traj_idx')['msd4'].rolling(window=11, min_periods=1, center=True).mean().reset_index(level=0, drop=True)\n        df['rolling_median4_15'] = df.groupby('traj_idx')['msd4'].rolling(window=15, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n        df['rolling_mean4_15'] = df.groupby('traj_idx')['msd4'].rolling(window=15, min_periods=1, center=True).mean().reset_index(level=0, drop=True)\n        df['rolling_mean4'] = df.groupby('traj_idx')['msd4'].rolling(window=5, min_periods=1, center=True).mean().reset_index(level=0, drop=True)\n        df['rolling_median4'] = df.groupby('traj_idx')['msd4'].rolling(window=5, min_periods=1, center=True).median().reset_index(level=0, drop=True)\n\n        # Calculate rolling alpha\n        df['rolling_alpha'] = np.log(df['rolling_median4_15'] / df['rolling_median1_15']) / np.log(4)\n        \n        # Concatenate the current dataframe to the main dataframe\n        big_df1 = pd.concat([big_df1, df], ignore_index=True)\n\n    # Convert 'frame' and 'traj_idx' columns to int32\n    big_df1.frame = big_df1.frame.astype(np.int32)\n    big_df1.traj_idx = big_df1.traj_idx.astype(np.int32)\n\n    # Initialize 'class' column\n    big_df1['class'] = 0\n\n    print(f\"exp={exp}\")\n    \n    # Apply Anderson statistic to msd1, msd2, msd3, and msd4 and gather results\n    anderson_results1 = big_df1.groupby(['traj_idx', 'fov'])['msd1'].apply(anderson_statistic).dropna().values\n    anderson_results2 = big_df1.groupby(['traj_idx', 'fov'])['msd2'].apply(anderson_statistic).dropna().values\n    anderson_results3 = big_df1.groupby(['traj_idx', 'fov'])['msd3'].apply(anderson_statistic).dropna().values\n    anderson_results4 = big_df1.groupby(['traj_idx', 'fov'])['msd4'].apply(anderson_statistic).dropna().values\n    \n    # Print Anderson statistic results\n    print(f\"msd1 anderson_results1 mean {anderson_results1.mean()}  std {anderson_results1.std()}   max {anderson_results1.max()}  q95 {np.quantile(anderson_results1, 0.95)}\")\n    print(f\"msd2 anderson_results2 mean {anderson_results2.mean()}  std {anderson_results2.std()}   max {anderson_results2.max()}  q95 {np.quantile(anderson_results2, 0.95)}\")\n    print(f\"msd3 anderson_results3 mean {anderson_results3.mean()}  std {anderson_results3.std()}   max {anderson_results3.max()}  q95 {np.quantile(anderson_results3, 0.95)}\")\n    print(f\"msd4 anderson_results4 mean {anderson_results4.mean()}  std {anderson_results4.std()}   max {anderson_results4.max()}  q95 {np.quantile(anderson_results4, 0.95)}\")\n\n    # Predict the model type\n    model_type_prediction = 'multi_state'  # Default model\n    if check_conf(big_df1[big_df1.fov == 1]):\n        model_type_prediction = 'confinement'\n    if np.quantile(anderson_results1, 0.95) < 2.5 and np.quantile(anderson_results2, 0.95) < 2.5 and np.quantile(anderson_results3, 0.95) < 2.5:\n        model_type_prediction = 'single_state'\n    if check_trap(big_df1):\n        model_type_prediction = 'immobile_traps'\n    if check_dim(big_df1):\n        model_type_prediction = 'dimerization'\n\n    print(f\"Model type predicted: {model_type_prediction}\")\n    model_type_predictions.append(model_type_prediction)\n\n    \n    # Re-initialize 'class' column\n    big_df1['class'] = 0\n    \n    # Calculate rolling alpha for msd3\n    big_df1['rolling_alpha_15'] = np.log(big_df1['rolling_median3_15'] / big_df1['rolling_median1_15']) / np.log(3)\n\n                                                                      \n    if model_type_prediction == 'dimerization':\n        x = big_df1.dist.values\n        y = big_df1.rolling_mean1_9.values\n\n        bin_width = 0.5\n        bins = np.arange(start=x.min(), stop=x.max() + bin_width, step=bin_width)\n        indmax = len(bins) - 2\n        \n        \n        indices = np.digitize(x, bins)  # Находит индексы корзин для каждого значения в x\n        y_0 = np.nanmean(y[indices == 1])\n        y_m = np.nanmean(y[indices == indmax])\n        y_tr = (y_0 + y_m) * 0.5\n        # Splitting x values into bins and calculating the average y values for each bin\n        indices = np.digitize(x, bins)  # Finds bin indices for each value in x\n        y_0 = np.nanmean(y[indices == 1])\n        y_m = np.nanmean(y[indices == indmax])\n        y_tr = (y_0 + y_m) * 0.5\n        indices = np.digitize(x, bins)  # Finds bin indices for each value in x        \n        y_means = [np.nanmean(y[indices == i]) for i in range(1, len(bins))]\n        y_means = np.array(y_means)\n        y_means = np.nan_to_num(y_means, nan=0.0)\n        plt.bar(bins[:-1], y_means, width=bin_width, align='edge', color='skyblue')\n\n        # Adding labels and title\n        plt.xlabel('x values')\n        plt.ylabel('Average y values')\n        plt.title('Histogram of average y values for x bins')\n\n        # Displaying the histogram\n        plt.show()            \n\n        bin_width = 0.2\n        bins = np.arange(start=x.min(), stop=x.max() + bin_width, step=bin_width)\n\n        # Splitting x values into bins and calculating the average y values for each bin\n        indices = np.digitize(x, bins)  # Finds bin indices for each value in x\n        y_means = [np.nanmean(y[indices == i]) for i in range(1, len(bins))]\n\n        # Removing NaN values for bins with no x elements\n        y_means = np.array(y_means)\n        y_means = np.nan_to_num(y_means, nan=0.0)\n\n        if y_0 < y_m:\n            r_min = bins[np.argmax(y_means[1:] > y_tr) + 1] if np.any(np.array(y_means) > y_tr) else None\n            new_col = (((x < r_min) + 0 + (y < y_tr) + 0) == 2) + 0\n        else:\n            r_min = bins[np.argmax(y_means[1:] < y_tr) + 1] if np.any(np.array(y_means) < y_tr) else None\n            new_col = (((x < r_min) + 0 + (y > y_tr) + 0) == 2) + 0\n        big_df1['class'] = fill_short_segments_np(1 - new_col, 3)\n\n        plt.bar(bins[:-1], y_means, width=bin_width, align='edge', color='skyblue')\n        print(f'r_min={r_min}, y_tr={y_tr}')\n        # Adding labels and title\n        plt.xlabel('x values')\n        plt.ylabel('Average y values')\n        plt.title('Histogram of average y values for x bins')\n\n        # Displaying the histogram\n        plt.show() \n\n    elif model_type_prediction == 'multi_state':\n        for fov in range(N_FOVS):\n            print(fov)\n            if exp == 10:\n                fov_class = div_by_rollstat(big_df1[big_df1['fov']==fov])\n            else:\n                fov_class = div_by_rollstat9(big_df1[big_df1['fov']==fov])\n            big_df1.loc[big_df1['fov'] == fov, 'class'] = fov_class\n    #         visualize_rolling_median(big_df1[big_df1['fov'] == fov])       \n\n    elif model_type_prediction == 'immobile_traps':\n        for fov in range(N_FOVS):\n            print(fov)\n            fov_class = divide_single_statistic_1only(big_df1[big_df1['fov'] == fov])\n            big_df1.loc[big_df1['fov'] == fov, 'class'] = fov_class\n\n    #         visualize_rolling_median(big_df1[big_df1['fov'] == fov]) \n\n    elif model_type_prediction == 'confinement' and exp == 5:\n    \n        for fov in range(N_FOVS):\n            print(fov)\n            fov_class = confined_circles5(big_df1[big_df1['fov']==fov], rol_stat='rolling_alpha', visualization = True, radius=7)   \n            big_df1.loc[big_df1['fov']==fov, 'class'] = fov_class\n    \n    elif model_type_prediction == 'confinement':\n        for fov in range(N_FOVS):\n            fov_class = confined_circles(big_df1[big_df1['fov']==fov], rol_stat='rolling_median3_9')\n            big_df1.loc[big_df1['fov'] == fov, 'class'] = fov_class\n\n    #         visualize_rolling_median(big_df1[big_df1['fov'] == fov]) \n\n    print(f\"big_df1['class'].unique() {big_df1['class'].unique()} \")\n    plt.hist(big_df1['rolling_alpha'], bins=100, range=[-1, 3], density=False)\n    plt.show()\n    visualize_traj(big_df1[big_df1['fov'] == 0])\n    visualize_rolling_median(big_df1[big_df1['fov'] == 0])\n\n    big_df1.reset_index(drop=True, inplace=True)\n    xy_norm = np.median([big_df1['x_diff'].abs().mean(), big_df1['y_diff'].abs().mean()])\n    big_df1.x /= xy_norm\n    big_df1.y /= xy_norm\n    big_df1['x_diff'] = big_df1.groupby(['traj_idx', 'fov', 'class'])['x'].diff(-1).ffill().bfill()\n    big_df1['y_diff'] = big_df1.groupby(['traj_idx', 'fov', 'class'])['y'].diff(-1).ffill().bfill()\n    big_df1['x_diff2'] = big_df1.groupby(['traj_idx', 'fov', 'class'])['x'].diff(-2).shift(-1).ffill().bfill()\n    big_df1['y_diff2'] = big_df1.groupby(['traj_idx', 'fov', 'class'])['y'].diff(-2).shift(-1).ffill().bfill()\n    big_df1['x_diff3'] = big_df1.groupby(['traj_idx', 'fov', 'class'])['x'].diff(-3).shift(-1).ffill().bfill()\n    big_df1['y_diff3'] = big_df1.groupby(['traj_idx', 'fov', 'class'])['y'].diff(-3).shift(-1).ffill().bfill()\n    big_df1['x_diff4'] = big_df1.groupby(['traj_idx', 'fov', 'class'])['x'].diff(-4).shift(-2).ffill().bfill()\n    big_df1['y_diff4'] = big_df1.groupby(['traj_idx', 'fov', 'class'])['y'].diff(-4).shift(-2).ffill().bfill()\n\n    big_df1['msd1'] = (big_df1['x_diff'] ** 2 + big_df1['y_diff'] ** 2) / 4\n    big_df1['msd2'] = (big_df1['x_diff2'] ** 2 + big_df1['y_diff2'] ** 2) / 4\n    big_df1['msd3'] = (big_df1['x_diff3'] ** 2 + big_df1['y_diff3'] ** 2) / 4\n    big_df1['msd4'] = (big_df1['x_diff4'] ** 2 + big_df1['y_diff4'] ** 2) / 4\n\n    msd1 = pd.pivot_table(big_df1, values='msd1', index=['fov', 'traj_idx', 'class'], columns='frame', dropna=False).reset_index()\n    msd2 = pd.pivot_table(big_df1, values='msd2', index=['fov', 'traj_idx', 'class'], columns='frame', dropna=False).reset_index()\n    msd3 = pd.pivot_table(big_df1, values='msd3', index=['fov', 'traj_idx', 'class'], columns='frame', dropna=False).reset_index()\n    msd4 = pd.pivot_table(big_df1, values='msd4', index=['fov', 'traj_idx', 'class'], columns='frame', dropna=False).reset_index()\n    \n    \n    #################### The KEY POINT of our METHOD ##############################################################\n    \n    # Calculate K as median for frames range and divide by log(2)\n    msd1['K'] = msd1.loc[:, range(1, N_FRAMES)].median(axis=1) / np.log(2)\n\n    # Calculate alpha using RowHurst function\n    msd1['alpha'] = RowHurst(msd3.loc[:, range(1, N_FRAMES)].values, msd1.loc[:, range(1, N_FRAMES)].values, 3)\n\n    ################## The End of key point of our method ############################################################\n\n    \n    msd1['cnt'] = msd1.loc[:, range(1, N_FRAMES)].count(axis=1)\n\n    \n    file_name = path_exp + 'ensemble_labels.txt'\n\n    with open(file_name, 'a') as f:\n        # Write model type and number of unique classes\n        f.write(f\"model: {model_type_prediction}; num_state: {len(big_df1['class'].unique())} \\n\")\n        data = np.random.rand(5, len(big_df1['class'].unique()))\n\n        for col in big_df1['class'].unique():\n            print(f'col={col}')\n            # Calculate Anderson-Darling statistics for each MSD\n            anderson_results1 = big_df1[big_df1['class']==col].groupby(['traj_idx', 'fov'])['msd1'].apply(anderson_statistic).dropna().values\n            anderson_results2 = big_df1[big_df1['class']==col].groupby(['traj_idx', 'fov'])['msd2'].apply(anderson_statistic).dropna().values\n            anderson_results3 = big_df1[big_df1['class']==col].groupby(['traj_idx', 'fov'])['msd3'].apply(anderson_statistic).dropna().values\n            anderson_results4 = big_df1[big_df1['class']==col].groupby(['traj_idx', 'fov'])['msd4'].apply(anderson_statistic).dropna().values\n\n            # Print mean, standard deviation, max, and 95th percentile for each MSD's Anderson-Darling statistics\n            print(f\"msd1 anderson_results1 mean {anderson_results1.mean()}  std {anderson_results1.std()}   max {anderson_results1.max()}  q95 {np.quantile(anderson_results1, 0.95)}\")\n            print(f\"msd2 anderson_results2 mean {anderson_results2.mean()}  std {anderson_results2.std()}   max {anderson_results2.max()}  q95 {np.quantile(anderson_results2, 0.95)}\")\n            print(f\"msd3 anderson_results1 mean {anderson_results3.mean()}  std {anderson_results3.std()}   max {anderson_results3.max()}  q95 {np.quantile(anderson_results3, 0.95)}\")\n            print(f\"msd4 anderson_results1 mean {anderson_results4.mean()}  std {anderson_results4.std()}   max {anderson_results4.max()}  q95 {np.quantile(anderson_results4, 0.95)}\")                \n\n            # Calculate the proportion of the current class\n            N2 = big_df1[big_df1['class']==col]['class'].count()            \n            N1 = big_df1['class'].count()         \n            print(f'col = {col}, % = {100*N2/N1:2f}')   \n\n            # Get dominant Gaussian parameters for K\n            K2, K_var2 = get_dominant_gaussian_params(msd1[msd1['class']==col]['K'], 2, 'K')\n            K = msd1[msd1['class']==col]['K'].median()\n            K3 = big_df1[big_df1['class']==col]['msd1'].median() / np.log(2)\n            K_var3 = big_df1[big_df1['class']==col]['rolling_median3_9'].var() / 3\n            K_var = msd1[msd1['class']==col]['K'].var() / 2\n\n            # Calculate alpha and its variance\n            alpha3 = msd1[msd1['class']==col]['alpha'].median()\n            alpha_var3 = np.nanmean((msd1[msd1['class']==col]['alpha']-alpha3) ** 2) / 3\n            if alpha3 > 0.6 and alpha3 < 1.4:\n                alpha = alpha3\n                alpha_var = alpha_var3\n            else:\n\n                alpha_med2, alpha1_var2 = get_dominant_gaussian_params(msd1[msd1['class']==col]['alpha'], 2, 'alpha')\n\n                alpha_med, alpha1_var = get_bounded_gaussian_params(msd1[msd1['class']==col]['alpha'])\n                alpha_var = (alpha1_var ** 2) / 3\n                alpha = (alpha_med + alpha_med2) / 2  \n\n\n\n            print(f'alpha3= {alpha3:4f}   alpha_var3={alpha_var3:4f}   K3={K3:4f}  K_var3={K_var3:4f}') \n            print(f'alpha= {alpha:4f}    alpha_var={alpha_var:4f}    K={K:4f}   K_var={K_var:4f}') \n\n            alpha = np.clip(alpha,0.00,1.999)\n            \n            # Store calculated data\n\n            data[2,col] = K * xy_norm**2\n            data[3,col] = K_var * xy_norm**2\n            data[0,col] = alpha\n            data[1,col] = alpha_var\n            data[4,col] = N2 / N1\n\n            if col == 0 and model_type_prediction == 'immobile_traps':\n                data[2,col] = 0\n                data[3,col] = 0\n                data[0,col] = 0\n                data[1,col] = 0\n\n\n        # Save the data in the corresponding ensemble file\n        np.savetxt(f, data, delimiter = ';') \n            \n            \n    # Process data for each field of view (FOV)        \n    for fov in range(N_FOVS):            \n            \n        dfx = msd1[msd1.fov == fov]\n        \n        df = big_df1[big_df1.fov == fov]\n            \n        submission_file = path_exp + f'fov_{fov}.txt'\n        traj_idx = df.traj_idx.unique()\n        \n        with open(submission_file, 'a') as f:\n            # Loop over each trajectory index\n            for idx in traj_idx:\n                \n                # Get the lenght of the trajectory\n                length_traj = df[df.traj_idx == idx].shape[0]\n              \n                traj_x = df[df.traj_idx == idx]['class'].values.astype(int)\n                # Array to save results\n                prediction_traj = [int(idx)]\n\n                # Initialize variables\n                current_class = traj_x[0]  # Initial class value\n                start_index = 0  # Start index of the current segment\n\n                # Iterate over all elements in traj_x\n                for i in range(1, len(traj_x)):\n                    # Check if the class has changed compared to the previous value\n                    if traj_x[i] != current_class:\n                        # Determine the length of the segment\n                        segment_length = i - start_index \n\n                        \n                        Kt = msd1[(msd1.traj_idx == idx)  & (msd1.fov == fov)  & (msd1['class'] == current_class)]['K'].max()  * xy_norm**2\n                        Nt = msd1[(msd1.traj_idx == idx)  & (msd1.fov == fov)  & (msd1['class'] == current_class)]['cnt'].max()\n                        if not Kt:\n                            Kt = data[2,current_class]\n                        elif current_class==0 and model_type_prediction == 'immobile_traps':\n                            Kt = data[2,current_class]\n                        else:\n                            Kt = (Kt * Nt + (400-Nt) * data[2,current_class]) / 400\n                            \n                        alphat = msd1[(msd1.traj_idx == idx)  & (msd1.fov == fov)  & (msd1['class'] == current_class)]['alpha'].max()  \n                        if not alphat:\n                            alphat = data[0,current_class]\n\n                        if current_class==0 and model_type_prediction == 'immobile_traps':\n                            alphat = data[0,current_class]\n                        else:\n                            alphat = (alphat * Nt + (400-Nt) * data[0,current_class]) / 400\n                        alphat = np.clip(alphat,0.00,1.99)\n                        \n                        prediction_traj += [Kt, alphat, get_freedom(current_class, alphat), i+1]\n\n                        # Update variables for the new segment\n                        current_class = traj_x[i]\n                        start_index = i  # Update the start index of the new segment\n\n                # Process the last segment\n                Kt = msd1[(msd1.traj_idx == idx)  & (msd1.fov == fov)  & (msd1['class'] == current_class)]['K'].max()  * xy_norm**2\n                Nt = msd1[(msd1.traj_idx == idx)  & (msd1.fov == fov)  & (msd1['class'] == current_class)]['cnt'].max()\n                if not Kt:\n                    Kt = data[2,current_class]\n                elif current_class==0 and model_type_prediction == 'immobile_traps':\n                    Kt = data[2,current_class]\n                else:\n                    Kt = (Kt * Nt + (400-Nt) * data[2,current_class]) / 400\n                    \n                alphat = msd1[(msd1.traj_idx == idx)  & (msd1.fov == fov)  & (msd1['class'] == current_class)]['alpha'].max() \n                if not alphat:\n                    alphat = data[0,current_class]\n                elif current_class==0 and model_type_prediction == 'immobile_traps':\n                    alphat = data[0,current_class]\n                else:\n                    alphat = (alphat * Nt + (400-Nt) * data[0,current_class]) / 400\n                alphat = np.clip(alphat,0.00,1.99)\n                segment_length = len(traj_x) - start_index\n                prediction_traj += [Kt, alphat, get_freedom(current_class, alphat), length_traj]\n\n\n                # Format and write the results\n\n                formatted_numbers = ','.join(map(str, prediction_traj))\n                f.write(formatted_numbers + '\\n')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Model type predicted: {model_type_predictions}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/res\", 'zip', \"/kaggle/working/res\")\nshutil.rmtree('/kaggle/working/res')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}